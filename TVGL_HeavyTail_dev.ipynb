{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6ef89497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyreadr\n",
    "from sklearn.preprocessing import scale\n",
    "from scipy.stats import t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "63709b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_price = pd.read_csv(\"rw_price.csv\")\n",
    "rw_returns = pd.read_csv(\"rw_returns.csv\")\n",
    "log_rw_returns = pd.read_csv(\"log_rw_returns.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ade654f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.34982739, 0.32699292, 0.27549521, 0.34886486],\n",
       "       [0.34982739, 1.        , 0.37785361, 0.33174046, 0.32484628],\n",
       "       [0.32699292, 0.37785361, 1.        , 0.33200448, 0.34886946],\n",
       "       [0.27549521, 0.33174046, 0.33200448, 1.        , 0.32947489],\n",
       "       [0.34886486, 0.32484628, 0.34886946, 0.32947489, 1.        ]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winLen = 200\n",
    "Nday = len(log_rw_returns)\n",
    "print(Nday)\n",
    "\n",
    "S_cov = np.corrcoef(scale(log_rw_returns).T)\n",
    "S_cov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5612a128",
   "metadata": {},
   "source": [
    "### Checking Graph operators from \n",
    "A Unified Framework for Structured Graph Learning via Spectral Constraints\n",
    "\n",
    "https://jmlr.org/papers/volume21/19-276/19-276.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2c1d452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_from_L(M: np.ndarray) -> np.ndarray:\n",
    "    M = np.asarray(M)\n",
    "    if M.ndim != 2 or M.shape[0] != M.shape[1]:\n",
    "        raise ValueError(\"M must be a square matrix\")\n",
    "\n",
    "    n = M.shape[0]\n",
    "    w = np.empty(n * (n - 1) // 2, dtype=M.dtype)\n",
    "    k = 0\n",
    "    for i in range(n - 1):            # i = 0 .. n-2\n",
    "        for j in range(i + 1, n):     # j = i+1 .. n-1 (strict upper)\n",
    "            w[k] = -M[i, j]\n",
    "            k += 1\n",
    "    return w\n",
    "\n",
    "def L_from_w(w: np.ndarray) -> np.ndarray:\n",
    "    w = np.asarray(w).ravel()\n",
    "    k = w.size\n",
    "    n = int((1 + np.sqrt(1 + 8 * k)) / 2)\n",
    "    if n * (n - 1) // 2 != k:\n",
    "        raise ValueError(\"Invalid length of w: must be n(n-1)/2 for some integer n.\")\n",
    "\n",
    "    Lw = np.zeros((n, n), dtype=float)\n",
    "\n",
    "    # Fill the strict upper triangle row by row, backwards like in C++\n",
    "    for i in range(n - 2, -1, -1):\n",
    "        j = n - i - 1\n",
    "        # Equivalent of Eigen: Lw.row(i).tail(j) = -w.head(k).tail(j)\n",
    "        Lw[i, -j:] = -w[:k][-j:]\n",
    "        k -= j\n",
    "\n",
    "    # Make symmetric: copy upper triangle into lower\n",
    "    Lw = Lw + Lw.T\n",
    "\n",
    "    # Adjust diagonal: subtract column sums\n",
    "    colsum = Lw.sum(axis=0)\n",
    "    np.fill_diagonal(Lw, Lw.diagonal() - colsum)\n",
    "\n",
    "    return Lw\n",
    "\n",
    "def W_init(M):\n",
    "    W0 = w_from_L(M)\n",
    "    W0[W0 < 0] = 0\n",
    "    return W0\n",
    "\n",
    "def L_star(Y):\n",
    "\n",
    "    Y = np.asarray(Y)\n",
    "    \n",
    "    if Y.ndim != 2 or Y.shape[0] != Y.shape[1]:\n",
    "        raise ValueError(\"Y must be a square matrix.\")\n",
    "    p = Y.shape[0]\n",
    "    Lstar = np.zeros(int(p*(p-1)/2))\n",
    "\n",
    "    for i in range(1,p+1):\n",
    "\n",
    "        for j in range(1,p+1):\n",
    "           \n",
    "            if (i > j):\n",
    "                k = int( (i - j) + ((j - 1)/2)*(2*p - j) ) \n",
    "                Lstar[k-1] = Y[i-1,i-1] - Y[i-1, j-1] - Y[ j-1, i-1] + Y[ j-1, j-1]\n",
    "\n",
    "\n",
    "    return Lstar\n",
    "\n",
    "def A_from_w(w):\n",
    "\n",
    "    w = np.asarray(w, dtype=float).ravel()\n",
    "    k = w.size\n",
    "\n",
    "    # Infer n from k = n*(n-1)/2\n",
    "    n_float = (1 + np.sqrt(1 + 8*k)) / 2\n",
    "    n = int(n_float)\n",
    "    if n*(n-1)//2 != k:\n",
    "        raise ValueError(f\"len(w)={k} is not a triangular number; expected k=n*(n-1)/2.\")\n",
    "\n",
    "    A = np.zeros((n, n), dtype=float)\n",
    "\n",
    "    # Indices of strict upper triangle in row-major order\n",
    "    iu = np.triu_indices(n, k=1)\n",
    "    A[iu] = w            # fill upper triangle\n",
    "    A[(iu[1], iu[0])] = w  # mirror to lower triangle\n",
    "    return A\n",
    "\n",
    "def w_from_A(M):\n",
    "    \n",
    "    N = M.shape[1]\n",
    "    k = N * (N - 1) // 2\n",
    "    w = np.zeros(k)\n",
    "    l = 0\n",
    "\n",
    "    for i in range(N - 1):\n",
    "        for j in range(i + 1, N):\n",
    "            w[l] = M[i, j]\n",
    "            l += 1\n",
    "    return w\n",
    "\n",
    "\n",
    "def A_star(Y):\n",
    "\n",
    "    Y = np.asarray(Y)\n",
    "    \n",
    "    if Y.ndim != 2 or Y.shape[0] != Y.shape[1]:\n",
    "        raise ValueError(\"Y must be a square matrix.\")\n",
    "    p = Y.shape[0]\n",
    "    Astar = np.zeros(int(p*(p-1)/2))\n",
    "\n",
    "    for i in range(1,p+1):\n",
    "\n",
    "        for j in range(1,p+1):\n",
    "           \n",
    "            if (i > j):\n",
    "                k = int( (i - j) + ((j - 1)/2)*(2*p - j) ) \n",
    "                Astar[k-1] = Y[i-1, j-1] + Y[ j-1, i-1]\n",
    "\n",
    "\n",
    "    return Astar\n",
    "\n",
    "def D_star(w: np.ndarray) -> np.ndarray:\n",
    "    w = np.asarray(w).ravel()\n",
    "    dStar = L_star(np.diag(w))\n",
    "    return  dStar\n",
    "\n",
    "def D_from_w(w: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return the column-wise sums of A(w).\n",
    "    Equivalent to Eigen: A(w).colwise().sum()\n",
    "    \"\"\"\n",
    "    M = A_from_w(w)               # shape (m, p)\n",
    "    return M.sum(axis=0)   # shape (p,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "69aad991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_cov\n",
      "[[1.         0.34982739 0.32699292 0.27549521 0.34886486]\n",
      " [0.34982739 1.         0.37785361 0.33174046 0.32484628]\n",
      " [0.32699292 0.37785361 1.         0.33200448 0.34886946]\n",
      " [0.27549521 0.33174046 0.33200448 1.         0.32947489]\n",
      " [0.34886486 0.32484628 0.34886946 0.32947489 1.        ]]\n",
      "S_prec\n",
      "[[ 1.27263721 -0.24947558 -0.19281752 -0.11937063 -0.25633942]\n",
      " [-0.24947558  1.32359962 -0.28620275 -0.21902718 -0.17092178]\n",
      " [-0.19281752 -0.28620275  1.32304704 -0.21528053 -0.23040203]\n",
      " [-0.11937063 -0.21902718 -0.21528053  1.25090233 -0.22424173]\n",
      " [-0.25633942 -0.17092178 -0.23040203 -0.22424173  1.29921337]]\n"
     ]
    }
   ],
   "source": [
    "log_rw_returns = pd.read_csv(\"log_rw_returns.csv\")\n",
    "\n",
    "S_cov = np.corrcoef(scale(log_rw_returns).T)\n",
    "print(\"S_cov\")\n",
    "print(S_cov)\n",
    "\n",
    "S_prec = np.linalg.pinv(S_cov)\n",
    "print(\"S_prec\")\n",
    "print(S_prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6050ed03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w from L\n",
      "[0.24947558 0.19281752 0.11937063 0.25633942 0.28620275 0.21902718\n",
      " 0.17092178 0.21528053 0.23040203 0.22424173]\n",
      "w\n",
      "[0.24947558 0.19281752 0.11937063 0.25633942 0.28620275 0.21902718\n",
      " 0.17092178 0.21528053 0.23040203 0.22424173]\n",
      "Lw from w\n",
      "[[ 0.81800315 -0.24947558 -0.19281752 -0.11937063 -0.25633942]\n",
      " [-0.24947558  0.9256273  -0.28620275 -0.21902718 -0.17092178]\n",
      " [-0.19281752 -0.28620275  0.92470283 -0.21528053 -0.23040203]\n",
      " [-0.11937063 -0.21902718 -0.21528053  0.77792007 -0.22424173]\n",
      " [-0.25633942 -0.17092178 -0.23040203 -0.22424173  0.88190495]]\n",
      "Lstar from Lw\n",
      "[2.24258162 2.12834103 1.83466448 2.21258694 2.42273564 2.14160172\n",
      " 2.14937582 2.13318397 2.26741184 2.10830848]\n",
      "Aw from w\n",
      "[[0.         0.24947558 0.19281752 0.11937063 0.25633942]\n",
      " [0.24947558 0.         0.28620275 0.21902718 0.17092178]\n",
      " [0.19281752 0.28620275 0.         0.21528053 0.23040203]\n",
      " [0.11937063 0.21902718 0.21528053 0.         0.22424173]\n",
      " [0.25633942 0.17092178 0.23040203 0.22424173 0.        ]]\n",
      "w from A\n",
      "[0.24947558 0.19281752 0.11937063 0.25633942 0.28620275 0.21902718\n",
      " 0.17092178 0.21528053 0.23040203 0.22424173]\n",
      "Astar from Aw\n",
      "[0.49895117 0.38563504 0.23874126 0.51267883 0.57240551 0.43805436\n",
      " 0.34184357 0.43056106 0.46080405 0.44848345]\n",
      "Dstar from w\n",
      "[0.4422931  0.36884621 0.505815   0.53567834 0.46850276 0.42039737\n",
      " 0.46475611 0.47987761 0.47371731 0.31218815 0.44915694 0.47902028\n",
      " 0.4118447  0.36373931 0.40809805 0.42321955 0.41705925 0.37571005\n",
      " 0.40557339 0.33839781 0.29029242 0.33465116 0.34977266 0.34361236\n",
      " 0.54254217 0.47536659 0.4272612  0.47161995 0.48674144 0.48058114\n",
      " 0.50522993 0.45712454 0.50148329 0.51660478 0.51044448 0.38994896\n",
      " 0.43430771 0.4494292  0.4432689  0.38620232 0.40132381 0.39516351\n",
      " 0.44568256 0.43952226 0.45464375]\n",
      "Dw from w\n",
      "[0.81800315 0.9256273  0.92470283 0.77792007 0.88190495]\n"
     ]
    }
   ],
   "source": [
    "wL = w_from_L(S_prec)\n",
    "print(\"w from L\")\n",
    "print(wL)\n",
    "\n",
    "w = W_init(S_prec)\n",
    "print(\"w\")\n",
    "print(w)\n",
    "\n",
    "Lw = L_from_w(w)\n",
    "print(\"Lw from w\")\n",
    "print(Lw) \n",
    "\n",
    "Lstar = L_star(Lw)\n",
    "print(\"Lstar from Lw\")\n",
    "print(Lstar) \n",
    "\n",
    "Aw = A_from_w(w)\n",
    "print('Aw from w')\n",
    "print(Aw)\n",
    "\n",
    "wA = w_from_A(Aw)\n",
    "print('w from A')\n",
    "print(wA)\n",
    "\n",
    "Astar = A_star(Aw)\n",
    "print('Astar from Aw')\n",
    "print(Astar)\n",
    "\n",
    "Dstar = D_star(w)\n",
    "print('Dstar from w')\n",
    "print(Dstar)\n",
    "\n",
    "Dw = D_from_w(w)\n",
    "print('Dw from w')\n",
    "print(Dw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb504f1",
   "metadata": {},
   "source": [
    "## Code From Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "5c7a21a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_rw_returns = pd.read_csv(\"log_rw_returns.csv\")\n",
    "\n",
    "X = scale(log_rw_returns[0:winLen].to_numpy())\n",
    "\n",
    "# number of nodes\n",
    "p = X.shape[1] # Predictors\n",
    "k = 1 # Clusters\n",
    "\n",
    "# number of observations\n",
    "T_n = X.shape[0]\n",
    "sigma_e = np.exp(0.1) # Student T Distribution Standard Dev. \n",
    "alpha = 2/(T_n*sigma_e)\n",
    "beta = 2*np.log(sigma_e)/T_n\n",
    "\n",
    "\n",
    "S_cov = np.corrcoef(X.T)\n",
    "S_prec = np.linalg.pinv(S_cov)\n",
    "\n",
    "w = W_init(S_prec)\n",
    "A0 = A_from_w(w)\n",
    "A0 = A0/np.sum(A0,axis=1,keepdims=True)\n",
    "w = w_from_A(A0)\n",
    "\n",
    "Lw = L_from_w(w)\n",
    "Aw = A_from_w(w)\n",
    "\n",
    "eigVals,eigVecs = np.linalg.eigh(Lw)\n",
    "U = eigVecs[:,p-k:]\n",
    "\n",
    "a0 = 1\n",
    "a = np.repeat(a0, p*(p-1)/2) \n",
    "w_lagged = np.repeat(0, p*(p-1)/2)\n",
    "\n",
    "Theta = Lw.copy()\n",
    "Phi_n = np.zeros((p,p))\n",
    "\n",
    "eta = 1e-8\n",
    "\n",
    "u_n = w - a*w_lagged\n",
    "mu_vec = np.repeat(0,p*(p-1)/2) # u_n ADMM dual\n",
    "\n",
    "d = 1 # degree Constraint\n",
    "z_n = np.repeat(0,p) # Degree Dual \n",
    "\n",
    "V = np.zeros((p,k))# p x k  {p = predictors, k = clusters}\n",
    "\n",
    "rho = 1                 # ADMM hyperparameter. \n",
    "\n",
    "mu = 2\n",
    "tau = 2\n",
    "\n",
    "# residual vectors\n",
    "primal_lap_residual = np.array([], dtype=float)\n",
    "primal_deg_residual = np.array([], dtype=float)\n",
    "dual_residual       = np.array([], dtype=float)\n",
    "\n",
    "lagrangian = np.array([], dtype=float)\n",
    "eta_seq = np.array([], dtype=float)\n",
    "\n",
    "elapsed_time = np.array([], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "4242d68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.78676739  0.16983348 -0.90957676  0.09089385 -0.13791795]\n",
      " [ 0.16983348  0.03666066 -0.1963434   0.01962056 -0.0297713 ]\n",
      " [-0.90957676 -0.1963434   1.0515559  -0.1050818   0.15944607]\n",
      " [ 0.09089385  0.01962056 -0.1050818   0.01050081 -0.01593342]\n",
      " [-0.13791795 -0.0297713   0.15944607 -0.01593342  0.0241766 ]]\n"
     ]
    }
   ],
   "source": [
    "def L_update( wn, Phi_n, rho, p, k):\n",
    "\n",
    "    ''' \n",
    "    L_n: Laplace Matrix (Ln = Diag(Wn1) - Wn = Lwn) Laplacian operator “A Unified Framework for Structured Graph Learning via Spectral Constraints”\n",
    "    Phi_n: ADMM Dual Variable \n",
    "    rho: ADMM Hyperparameter\n",
    "    p: Number of time series\n",
    "    k: Clusters\n",
    "    '''\n",
    "    # Reference: Equation 16\n",
    "    \n",
    "    Y = L_from_w(wn) + Phi_n/rho\n",
    "\n",
    "    eigVals,eigVecs = np.linalg.eigh(Y)\n",
    "    U = eigVecs[:,p-k:]\n",
    "    R = np.diag(eigVals[p-k:])\n",
    "    I = np.eye(R.shape[0]) \n",
    "    Ln = 0.5 * U @ ( R + np.sqrt(  R**2 + 4/rho * I  ) ) @ U.T\n",
    "    return Ln\n",
    "\n",
    "\n",
    "Ln = L_update( w, Phi_n, rho, p, k)\n",
    "print(Ln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "93f872b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 5)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import multivariate_t\n",
    "\n",
    "# mean (mu)\n",
    "loc = [0., 0., 0., 0., 0.]\n",
    "\n",
    "# covariance (Sigma)\n",
    "shape = [\n",
    "    [1.5,     0.75,    0.375,   0.1875,  0.09375],\n",
    "    [0.75,    1.5,     0.75,    0.375,   0.1875 ],\n",
    "    [0.375,   0.75,    1.5,     0.75,    0.375  ],\n",
    "    [0.1875,  0.375,   0.75,    1.5,     0.75   ],\n",
    "    [0.09375, 0.1875,  0.375,   0.75,    1.5    ],\n",
    "]\n",
    "\n",
    "# degrees of freedom\n",
    "df = 6.0\n",
    "\n",
    "# Create a frozen multivariate_t object\n",
    "# This allows fixing the parameters and then calling methods like rvs()\n",
    "dist = multivariate_t(loc=loc, shape=shape, df=df)\n",
    "\n",
    "# Generate samples\n",
    "num_samples = 1000\n",
    "samples = dist.rvs(size=num_samples)\n",
    "samples.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "ea9bf5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu: [-0.02766317 -0.07871325  0.0247867   0.02145099 -0.00145958]\n",
      "nu: 6.377364535231853\n",
      "Sigma:\n",
      " [[1.43181152 0.62514321 0.31916069 0.13240914 0.09206454]\n",
      " [0.62514321 1.38117318 0.69953388 0.35969955 0.23922112]\n",
      " [0.31916069 0.69953388 1.46624374 0.73770786 0.46042783]\n",
      " [0.13240914 0.35969955 0.73770786 1.49563501 0.74303463]\n",
      " [0.09206454 0.23922112 0.46042783 0.74303463 1.42116393]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import slogdet\n",
    "from scipy.special import digamma, polygamma, gammaln\n",
    "# Reference\n",
    "# https://zouyuxin.github.io/Note/EMtDistribution.pdf\n",
    "# https://shoichimidorikawa.github.io/Lec/ProbDistr/t-e.pdf\n",
    "def fit_multivariate_t(\n",
    "    X,\n",
    "    nu_init=10.0,\n",
    "    max_iter=200,\n",
    "    tol=1e-6,\n",
    "    fix_nu=None,\n",
    "    jitter=1e-6,\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Estimate (mu, Sigma, nu) for a multivariate Student's t via EM.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape (n_samples, n_features)\n",
    "        Data.\n",
    "    nu_init : float\n",
    "        Initial degrees of freedom (ignored if fix_nu is not None).\n",
    "    max_iter : int\n",
    "        Maximum EM iterations.\n",
    "    tol : float\n",
    "        Relative tolerance on log-likelihood for convergence.\n",
    "    fix_nu : float or None\n",
    "        If set, keep nu fixed at this value.\n",
    "    jitter : float\n",
    "        Diagonal jitter multiplier for numerical stability.\n",
    "    verbose : bool\n",
    "        Print progress if True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mu : array, shape (p,)\n",
    "    Sigma : array, shape (p, p)\n",
    "    nu : float\n",
    "    history : dict with 'loglik'\n",
    "    \"\"\"\n",
    "    X = np.asarray(X)\n",
    "    n, p = X.shape\n",
    "\n",
    "    # Initialize\n",
    "    mu = X.mean(axis=0)\n",
    "    # sample covariance (unbiased=False), add tiny jitter\n",
    "    centered0 = X - mu\n",
    "    Sigma = centered0.T @ centered0 / n\n",
    "    Sigma += np.eye(p) * (jitter * np.trace(Sigma) / p + 1e-12)\n",
    "    nu = float(nu_init if fix_nu is None else fix_nu)\n",
    "\n",
    "    def mahal_sq(Sigma, X, mu):\n",
    "        # δ_i = (x_i - mu)^T Sigma^{-1} (x_i - mu)\n",
    "        L = np.linalg.cholesky(Sigma)\n",
    "        Y = np.linalg.solve(L, (X - mu).T)   # shape (p, n)\n",
    "        return np.sum(Y * Y, axis=0)         # shape (n,)\n",
    "\n",
    "    def loglik(mu, Sigma, nu):\n",
    "        # log-likelihood of multivariate t\n",
    "        sgn, logdet = slogdet(Sigma)\n",
    "        if sgn <= 0:\n",
    "            return -np.inf\n",
    "        delta = mahal_sq(Sigma, X, mu)\n",
    "        term1 = gammaln((nu + p) / 2) - gammaln(nu / 2)\n",
    "        term2 = - (p / 2) * np.log(nu * np.pi) - 0.5 * logdet\n",
    "        term3 = - ((nu + p) / 2) * np.log1p(delta / nu)\n",
    "        return np.sum(term1 + term2 + term3)\n",
    "\n",
    "    ll_old = -np.inf\n",
    "    history = {'loglik': []}\n",
    "\n",
    "    for it in range(1, max_iter + 1):\n",
    "        # E-step\n",
    "        delta = mahal_sq(Sigma, X, mu)\n",
    "        w = (nu + p) / (nu + delta)  # E[lambda_i | x_i]\n",
    "        eloglam = digamma((nu + p) / 2) - np.log((nu + delta) / 2)  # E[log lambda_i | x_i]\n",
    "\n",
    "        # M-step: mu (weighted mean)\n",
    "        sumw = w.sum()\n",
    "        mu = (w[:, None] * X).sum(axis=0) / sumw\n",
    "\n",
    "        # M-step: Sigma\n",
    "        Xc = X - mu\n",
    "        Sigma = (Xc.T * w) @ Xc / n\n",
    "        # stabilize\n",
    "        Sigma += np.eye(p) * (jitter * np.trace(Sigma) / p + 1e-12)\n",
    "\n",
    "        # M-step: nu (solve for root of dQ/dnu = 0) unless fixed\n",
    "        if fix_nu is None:\n",
    "            # f(nu) = log(nu/2) - psi(nu/2) + 1 + (1/n) * sum(E[log lambda_i] - E[lambda_i]) = 0\n",
    "            c = (eloglam.mean() - w.mean())\n",
    "\n",
    "            def f(nu_):\n",
    "                return np.log(nu_ / 2.0) - digamma(nu_ / 2.0) + 1.0 + c\n",
    "\n",
    "            def fprime(nu_):\n",
    "                return 1.0 / nu_ - 0.5 * polygamma(1, nu_ / 2.0)\n",
    "\n",
    "            # Newton update with simple guarding\n",
    "            nu_new = max(nu, 2.01)  # keep > 2 so covariance exists\n",
    "            for _ in range(30):\n",
    "                val = f(nu_new)\n",
    "                der = fprime(nu_new)\n",
    "                step = val / der\n",
    "                nu_new = nu_new - step\n",
    "                if nu_new < 2.01:\n",
    "                    nu_new = 2.01\n",
    "                if abs(step) / nu_new < 1e-6:\n",
    "                    break\n",
    "            nu = float(nu_new)\n",
    "        else:\n",
    "            nu = float(fix_nu)\n",
    "\n",
    "        # Evaluate log-likelihood and check convergence\n",
    "        ll = loglik(mu, Sigma, nu)\n",
    "        history['loglik'].append(ll)\n",
    "        if verbose:\n",
    "            print(f\"iter {it:3d}: ll={ll:.6f}, nu={nu:.4f}\")\n",
    "\n",
    "        if np.isfinite(ll_old):\n",
    "            if abs(ll - ll_old) / (abs(ll_old) + 1e-12) < tol:\n",
    "                break\n",
    "        ll_old = ll\n",
    "\n",
    "    return mu, Sigma, nu, history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# X: (n_samples, n_features) data\n",
    "\n",
    "mu, Sigma, nu, history = fit_multivariate_t(\n",
    "samples,\n",
    "nu_init=30.0,\n",
    "max_iter=200,\n",
    "tol=1e-6,\n",
    "fix_nu=None,\n",
    "jitter=1e-6,\n",
    "verbose=False,\n",
    ")\n",
    "print(\"mu:\", mu)\n",
    "print(\"nu:\", nu)\n",
    "print(\"Sigma:\\n\", Sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "b587857c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001] ll=-11529.219245, mu=0.005, lam=0.299, nu=6.408\n",
      "[002] ll=-11491.897471, mu=0.003, lam=0.261, nu=6.053\n",
      "[003] ll=-11488.705740, mu=0.003, lam=0.252, nu=5.914\n",
      "[004] ll=-11488.126385, mu=0.003, lam=0.250, nu=5.830\n",
      "[005] ll=-11487.790087, mu=0.003, lam=0.250, nu=5.765\n",
      "[006] ll=-11487.506431, mu=0.003, lam=0.250, nu=5.707\n",
      "[007] ll=-11487.256721, mu=0.003, lam=0.251, nu=5.655\n",
      "[008] ll=-11487.036339, mu=0.003, lam=0.252, nu=5.606\n",
      "[009] ll=-11486.842075, mu=0.003, lam=0.253, nu=5.561\n",
      "[010] ll=-11486.671086, mu=0.003, lam=0.253, nu=5.519\n",
      "[011] ll=-11486.520800, mu=0.003, lam=0.254, nu=5.480\n",
      "[012] ll=-11486.388888, mu=0.003, lam=0.254, nu=5.444\n",
      "[013] ll=-11486.273251, mu=0.003, lam=0.255, nu=5.411\n",
      "[014] ll=-11486.172002, mu=0.003, lam=0.255, nu=5.379\n",
      "[015] ll=-11486.083449, mu=0.003, lam=0.256, nu=5.350\n",
      "[016] ll=-11486.006083, mu=0.003, lam=0.256, nu=5.323\n",
      "[017] ll=-11485.938557, mu=0.002, lam=0.257, nu=5.298\n",
      "[018] ll=-11485.879673, mu=0.002, lam=0.257, nu=5.275\n",
      "[019] ll=-11485.828371, mu=0.002, lam=0.258, nu=5.253\n",
      "[020] ll=-11485.783709, mu=0.002, lam=0.258, nu=5.233\n",
      "[021] ll=-11485.744859, mu=0.002, lam=0.258, nu=5.214\n",
      "[022] ll=-11485.711087, mu=0.002, lam=0.259, nu=5.197\n",
      "[023] ll=-11485.681750, mu=0.002, lam=0.259, nu=5.180\n",
      "[024] ll=-11485.656280, mu=0.002, lam=0.259, nu=5.165\n",
      "[025] ll=-11485.634181, mu=0.002, lam=0.259, nu=5.152\n",
      "[026] ll=-11485.615016, mu=0.002, lam=0.260, nu=5.139\n",
      "[027] ll=-11485.598405, mu=0.002, lam=0.260, nu=5.127\n",
      "[028] ll=-11485.584014, mu=0.002, lam=0.260, nu=5.115\n",
      "[029] ll=-11485.571551, mu=0.002, lam=0.260, nu=5.105\n",
      "[030] ll=-11485.560763, mu=0.002, lam=0.260, nu=5.095\n",
      "\n",
      "Estimated parameters:\n",
      "mu = 0.0022766084306091163\n",
      "sigma^2 ≈ 1.9598020118034953\n",
      "nu = 5.095468978675405\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import digamma, polygamma, gammaln\n",
    "\n",
    "def _update_nu_univariate(E_eta, E_log_eta, nu0, max_iter=50, tol=1e-8):\n",
    "    target = 1.0 + np.mean(E_log_eta) - np.mean(E_eta)\n",
    "    nu = max(float(nu0), 1e-6)\n",
    "    for _ in range(max_iter):\n",
    "        f  = digamma(nu/2.0) - np.log(nu/2.0) - target\n",
    "        df = 0.5 * polygamma(1, nu/2.0) - 1.0/nu\n",
    "        step = f / df\n",
    "        nu_new = max(nu - step, 1e-8)\n",
    "        if abs(nu_new - nu) < tol * (1.0 + abs(nu)):\n",
    "            return nu_new\n",
    "        nu = nu_new\n",
    "    return nu\n",
    "\n",
    "def em_student_t_univariate(x, mu=None, lam=None, nu=10.0,\n",
    "                            max_iter=200, tol=1e-6, verbose=False):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    N = x.shape[0]\n",
    "\n",
    "    if mu  is None: mu  = x.mean()\n",
    "    if lam is None: lam = 1.0 / np.var(x)   # precision of t's scale s^2 (not variance)\n",
    "    nu = float(nu)\n",
    "\n",
    "    loglik_trace = []\n",
    "    prev_ll = -np.inf\n",
    "\n",
    "    for it in range(1, max_iter+1):\n",
    "        # Save old params for param-change stop\n",
    "        mu_prev, lam_prev, nu_prev = mu, lam, nu\n",
    "\n",
    "        # -------- E-step --------\n",
    "        q = lam * (x - mu)**2\n",
    "        E_eta     = (nu + 1.0) / (nu + q)\n",
    "        E_log_eta = digamma((nu + 1.0)/2.0) - np.log((nu + q)/2.0)\n",
    "\n",
    "        # -------- M-step --------\n",
    "        mu  = np.sum(E_eta * x) / np.sum(E_eta)\n",
    "        lam = N / np.sum(E_eta * (x - mu)**2)\n",
    "        nu  = _update_nu_univariate(E_eta, E_log_eta, nu)\n",
    "\n",
    "        # -------- Recompute q with UPDATED params --------\n",
    "        q = lam * (x - mu)**2\n",
    "\n",
    "        # -------- Log-likelihood with UPDATED params --------\n",
    "        ll = (\n",
    "            gammaln((nu + 1.0)/2.0) - gammaln(nu/2.0)\n",
    "            + 0.5*np.log(lam / (np.pi * nu))\n",
    "            - ((nu + 1.0)/2.0) * np.log1p(q / nu)\n",
    "        ).sum()\n",
    "        loglik_trace.append(ll)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[{it:03d}] ll={ll:.6f}, mu={mu:.3f}, lam={lam:.3f}, nu={nu:.3f}\")\n",
    "\n",
    "        # Convergence: ΔLL and parameter change\n",
    "        param_change = max(abs(mu - mu_prev), abs(lam - lam_prev), abs(nu - nu_prev))\n",
    "        if (np.isfinite(prev_ll) and ll - prev_ll < tol * (1.0 + abs(prev_ll))) or (param_change < 1e-7):\n",
    "            break\n",
    "        prev_ll = ll\n",
    "\n",
    "    return mu, lam, nu, np.array(loglik_trace)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import t\n",
    "\n",
    "# --- Generate synthetic univariate Student-t data ---\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "N = 5000\n",
    "true_mu = 0.0     # location\n",
    "true_sigma = 2.0  # scale (std dev-like, not precision)\n",
    "true_nu = 5.0     # degrees of freedom\n",
    "\n",
    "# scipy's t takes df=nu, loc=mu, scale=sigma\n",
    "x = t.rvs(df=true_nu, loc=true_mu, scale=true_sigma, size=N, random_state=rng)\n",
    "\n",
    "\n",
    "# Fit with EM\n",
    "mu_hat, lam_hat, nu_hat, ll = em_student_t_univariate(x, mu=None, lam=0.5, nu=8,\n",
    "                            max_iter=200, tol=1e-6, verbose=True)\n",
    "print(\"\\nEstimated parameters:\")\n",
    "print(\"mu =\", mu_hat)\n",
    "print(\"sigma^2 ≈\", np.sqrt(1/lam_hat))\n",
    "print(\"nu =\", nu_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "7c36b164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True mu:\n",
      " [0. 0. 0. 0. 0.]\n",
      "\n",
      "True precision Lambda:\n",
      " [[ 8.88888889e-01 -4.44444444e-01 -3.70074342e-17 -1.85037171e-17\n",
      "   0.00000000e+00]\n",
      " [-4.44444444e-01  1.11111111e+00 -4.44444444e-01  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00 -4.44444444e-01  1.11111111e+00 -4.44444444e-01\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00 -4.44444444e-01  1.11111111e+00\n",
      "  -4.44444444e-01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -4.44444444e-01\n",
      "   8.88888889e-01]]\n",
      "\n",
      "True nu:\n",
      " 6.0\n",
      "\n",
      "Estimated mu:\n",
      " [ 0.00335245  0.00319906  0.00281707  0.0007678  -0.00179385]\n",
      "\n",
      "Estimated precision Lambda:\n",
      " [[ 8.91504286e-01 -4.49066319e-01  3.61762082e-03  5.43643906e-04\n",
      "   7.20005726e-04]\n",
      " [-4.49066319e-01  1.11777088e+00 -4.49207893e-01  4.62460191e-03\n",
      "  -3.25564965e-03]\n",
      " [ 3.61762082e-03 -4.49207893e-01  1.11911552e+00 -4.51158122e-01\n",
      "   2.34583100e-03]\n",
      " [ 5.43643906e-04  4.62460191e-03 -4.51158122e-01  1.11597787e+00\n",
      "  -4.46948765e-01]\n",
      " [ 7.20005726e-04 -3.25564965e-03  2.34583100e-03 -4.46948765e-01\n",
      "   8.94892084e-01]]\n",
      "\n",
      "Estimated nu:\n",
      " 5.965999879585947\n",
      "\n",
      "Relative Frobenius error vs true Λ: 0.008262066700169313\n",
      "Max |mu_hat - mu_true|: 0.003352448175504514\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv, pinv, slogdet, LinAlgError\n",
    "from scipy.special import digamma, polygamma, gammaln\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def _quadforms(X, mu, Lambda):\n",
    "    \"\"\"q_i = (x_i - mu)^T Lambda (x_i - mu) for all rows.\"\"\"\n",
    "    diff = X - mu[None, :]\n",
    "    y = diff @ Lambda  # (N,D)\n",
    "    return np.einsum(\"ni,ni->n\", diff, y)\n",
    "\n",
    "def _loglik_student_t(X, mu, Lambda, nu):\n",
    "    \"\"\"Total log-likelihood under multivariate Student-t (precision form).\"\"\"\n",
    "    N, D = X.shape\n",
    "    q = _quadforms(X, mu, Lambda)\n",
    "    sign, logdet_L = slogdet(Lambda)\n",
    "    if sign <= 0:\n",
    "        return -np.inf\n",
    "    c = (\n",
    "        gammaln((nu + D) / 2.0)\n",
    "        - gammaln(nu / 2.0)\n",
    "        + 0.5 * logdet_L\n",
    "        - (D / 2.0) * np.log(nu * np.pi)\n",
    "    )\n",
    "    return np.sum(c - 0.5 * (nu + D) * np.log1p(q / nu))\n",
    "\n",
    "def _update_nu(E_eta, E_log_eta, nu0, max_iter=50, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Newton–Raphson for: psi(nu/2) - log(nu/2) = 1 + mean(E[log eta]) - mean(E[eta]).\n",
    "    \"\"\"\n",
    "    target = 1.0 + float(np.mean(E_log_eta)) - float(np.mean(E_eta))\n",
    "    nu = max(float(nu0), 1e-6)\n",
    "    for _ in range(max_iter):\n",
    "        f  = digamma(nu / 2.0) - np.log(nu / 2.0) - target\n",
    "        df = 0.5 * polygamma(1, nu / 2.0) - 1.0 / nu\n",
    "        step = f / df\n",
    "        nu_new = max(nu - step, 1e-8)\n",
    "        if abs(nu_new - nu) < tol * (1.0 + abs(nu)):\n",
    "            return nu_new\n",
    "        nu = nu_new\n",
    "    return nu\n",
    "\n",
    "def _safe_inv(S):\n",
    "    \"\"\"Try inv, fall back to pinv if S is near-singular.\"\"\"\n",
    "    try:\n",
    "        return inv(S)\n",
    "    except LinAlgError:\n",
    "        return pinv(S)\n",
    "\n",
    "# ---------- EM main ----------\n",
    "def em_multivariate_student_t(\n",
    "    X, mu=None, Lambda=None, nu=10.0,\n",
    "    max_iter=200, tol=1e-6, jitter=1e-6, verbose=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Fit a multivariate Student's t via EM (precision parameterization).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : (N,D) array\n",
    "        Data.\n",
    "    mu : (D,) or None\n",
    "        Initial mean (defaults to sample mean).\n",
    "    Lambda : (D,D) or None\n",
    "        Initial precision (defaults to inverse sample covariance).\n",
    "    nu : float\n",
    "        Initial degrees of freedom (>0).\n",
    "    max_iter : int\n",
    "        Max EM iterations.\n",
    "    tol : float\n",
    "        Relative LL tolerance for convergence.\n",
    "    jitter : float\n",
    "        Base jitter added to the scatter before inversion (stability).\n",
    "    verbose : bool\n",
    "        Print progress if True.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    mu, Lambda, nu, loglik_trace\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    N, D = X.shape\n",
    "\n",
    "    # --- init ---\n",
    "    if mu is None:\n",
    "        mu = X.mean(axis=0)\n",
    "    if Lambda is None:\n",
    "        S0 = np.cov(X.T, bias=False)\n",
    "        S0 = 0.5 * (S0 + S0.T)\n",
    "        S0 += jitter * np.trace(S0) / D * np.eye(D)\n",
    "        Lambda = _safe_inv(S0)\n",
    "    nu = float(nu)\n",
    "\n",
    "    loglik_trace = []\n",
    "    prev_ll = -np.inf\n",
    "\n",
    "    for it in range(1, max_iter + 1):\n",
    "        mu_old, Lambda_old, nu_old = mu.copy(), Lambda.copy(), nu\n",
    "\n",
    "        # ----- E-step -----\n",
    "        q = _quadforms(X, mu, Lambda)              # (N,)\n",
    "        E_eta = (nu + D) / (nu + q)                # weights E[eta_i]\n",
    "        E_log_eta = digamma((nu + D) / 2.0) - np.log((nu + q) / 2.0)\n",
    "\n",
    "        # ----- M-step -----\n",
    "        # mean (weighted)\n",
    "        wsum = E_eta.sum()\n",
    "        mu = (X * E_eta[:, None]).sum(axis=0) / wsum\n",
    "\n",
    "        # weighted scatter -> precision\n",
    "        diff = X - mu[None, :]\n",
    "        S = (diff * E_eta[:, None]).T @ diff / N\n",
    "        # symmetrize and stabilize\n",
    "        S = 0.5 * (S + S.T)\n",
    "        S += jitter * np.trace(S) / D * np.eye(D)\n",
    "        Lambda = _safe_inv(S)\n",
    "        # symmetrize Lambda too (tiny numerical asymmetries)\n",
    "        Lambda = 0.5 * (Lambda + Lambda.T)\n",
    "\n",
    "        # update nu (scalar)\n",
    "        nu = _update_nu(E_eta, E_log_eta, nu)\n",
    "\n",
    "        # ----- log-likelihood with updated params -----\n",
    "        ll = _loglik_student_t(X, mu, Lambda, nu)\n",
    "        loglik_trace.append(ll)\n",
    "        if verbose:\n",
    "            print(f\"[{it:03d}] ll={ll:.6f}  nu={nu:.4f}\")\n",
    "\n",
    "        # convergence: ΔLL and parameter movement\n",
    "        param_delta = max(\n",
    "            np.linalg.norm(mu - mu_old, ord=np.inf),\n",
    "            np.linalg.norm(Lambda - Lambda_old, ord='fro') / (1e-12 + np.linalg.norm(Lambda_old, ord='fro')),\n",
    "            abs(nu - nu_old) / (1e-12 + abs(nu_old))\n",
    "        )\n",
    "        if (np.isfinite(prev_ll) and ll - prev_ll < tol * (1.0 + abs(prev_ll))) or (param_delta < 1e-7):\n",
    "            break\n",
    "        prev_ll = ll\n",
    "\n",
    "    return mu, Lambda, nu, np.array(loglik_trace)\n",
    "\n",
    "\n",
    "# -------------------- demo / usage --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    from scipy.stats import multivariate_t\n",
    "\n",
    "    # mean (mu)\n",
    "    loc = np.zeros(5)\n",
    "\n",
    "    # \"shape\" matrix (Σ) used by scipy's multivariate_t (this is NOT the covariance)\n",
    "    shape = np.array([\n",
    "        [1.5,     0.75,    0.375,   0.1875,  0.09375],\n",
    "        [0.75,    1.5,     0.75,    0.375,   0.1875 ],\n",
    "        [0.375,   0.75,    1.5,     0.75,    0.375  ],\n",
    "        [0.1875,  0.375,   0.75,    1.5,     0.75   ],\n",
    "        [0.09375, 0.1875,  0.375,   0.75,    1.5    ],\n",
    "    ])\n",
    "    Lambda_true = inv(shape)   # precision in the EM model\n",
    "    df = 6.0\n",
    "\n",
    "    print(\"\\nTrue mu:\\n\", loc)\n",
    "    print(\"\\nTrue precision Lambda:\\n\", Lambda_true)\n",
    "    print(\"\\nTrue nu:\\n\", df)\n",
    "\n",
    "    # Generate samples\n",
    "    rng = np.random.default_rng(0)\n",
    "    dist = multivariate_t(loc=loc, shape=shape, df=df, seed=rng)\n",
    "    num_samples = 200000  # more data → better ν\n",
    "    samples = dist.rvs(size=num_samples)\n",
    "\n",
    "    # Fit\n",
    "    mu_hat, Lambda_hat, nu_hat, ll = em_multivariate_student_t(\n",
    "        samples, mu=None, Lambda=None, nu=8.0,\n",
    "        max_iter=300, tol=1e-10, jitter=1e-6, verbose=False\n",
    "    )\n",
    "\n",
    "    print(\"\\nEstimated mu:\\n\", mu_hat)\n",
    "    print(\"\\nEstimated precision Lambda:\\n\", Lambda_hat)\n",
    "    print(\"\\nEstimated nu:\\n\", nu_hat)\n",
    "\n",
    "    # simple diagnostics\n",
    "    rel_err = np.linalg.norm(Lambda_hat - Lambda_true, 'fro') / np.linalg.norm(Lambda_true, 'fro')\n",
    "    print(\"\\nRelative Frobenius error vs true Λ:\", rel_err)\n",
    "    print(\"Max |mu_hat - mu_true|:\", np.max(np.abs(mu_hat - loc)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0084d5f9",
   "metadata": {},
   "source": [
    "### w update "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "d610c8e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0.])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_sum = 0\n",
    "x = samples[0:winLen,:]\n",
    "Lw = L_from_w(w)\n",
    "\n",
    "# can be replaced by an einsum\n",
    "for t in range(0,winLen):\n",
    "    S_sum += ( np.outer(x[t,:],x[t,:]) )/( x[t,:].T @ Lw @ x[t,:] + nu )\n",
    "C = (p + nu)/winLen\n",
    "S_tilde = C * S_sum\n",
    "\n",
    "\n",
    "aw = L_star(S_tilde + Phi_n + rho*(Lw - Ln) + eta*np.outer(V,V.T) )\n",
    "bw = -u_n - rho*( u_n + np.multiply(a,w_lagged) ) + D_star(z_n - rho*(d - D_from_w(w) ))\n",
    "cw = (1 - rho/(rho*(4*rho -1) ) ) * w - 1/( rho*(4*rho - 1) ) * (aw + bw)\n",
    "cth = np.sqrt((2*beta)/(rho*(4*rho - 1))) * np.repeat(1,p*(p-1)/2)\n",
    "\n",
    "Wupdate = np.multiply((cw > cth),cw)\n",
    "Wupdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "5672287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softThresh(v,thr):\n",
    "    temp = abs(v) - thr\n",
    "    temp = temp*(temp>0)\n",
    "    return(np.sign(v)*temp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae50cde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_update( x, beta, rho, a, w_n_prev, w_n, L_n, Vn, p, k, d, Tn, nu, eta, Phi_n, mu_n, z_n):\n",
    "\n",
    "    ''' \n",
    "    x: observation for each time series\n",
    "    p: Number of Time Series\n",
    "    a: VAR coefficients\n",
    "    k: Number of Clusters\n",
    "    nu: Student T degrees of freedom\n",
    "    Vn: Laplacian spectral constraint\n",
    "    Tn: Number of observations in a Frame \n",
    "    eta: regularization term\n",
    "    d: Node degree constraint\n",
    "\n",
    "    Phi_n: ADMM dual variable\n",
    "    mu_n: ADMM dual variable\n",
    "    z_n: ADMM dual variable\n",
    "    '''\n",
    "\n",
    "    assert Vn.shape[0] == p and Vn.shape[1] == k\n",
    "\n",
    "    Lstar = L_star(w) \n",
    "    dstar = D_star(w)\n",
    "\n",
    "    u_n = w_n - a * w_n_prev # Vector Autoregressive variable\n",
    "\n",
    "    cth = np.sqrt( (2*beta)/(rho*(4*rho -1)) ) * np.ones(len(w_n))\n",
    "\n",
    "    S_sum = 0\n",
    "    for i in range(0,Tn):\n",
    "        S_sum += ( x[i,:]*x[i,:].T )/( x[i,:].T @ L_n @ x[i,:] + nu )\n",
    "\n",
    "    S = ( (p + nu)/Tn ) * S_sum\n",
    "\n",
    "    aw = L_star @ ( S + Phi_n + rho*(L_star - L_n) + eta * Vn@Vn.T )\n",
    "\n",
    "    bw = -mu_n - rho*( u_n + a*w_n_prev ) + d_star @ ( z_n - rho * ( d - d_star*w_n ) )\n",
    "\n",
    "    cw = (1 - (rho)/(rho*(4*p-1))) * wn - (1)/( rho*(4*p -1) ) * (aw + bw)\n",
    "\n",
    "    wn = (cw > cth) * cw\n",
    "\n",
    "    return wn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "convexfi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
